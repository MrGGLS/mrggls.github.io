---
layout: homepage
---

## About Me

Hello, I'm Longguang Zhong, an M.S. student in Computer Technology at Sun Yat-sen University, where I'm currently studying under the supervision of Prof. [Xiaojun Quan](https://sites.google.com/site/xiaojunquan/). I earned my bachelor's degree in Software Engineering from Xidian University.

## Research Interests

 My research focuses on large language models, including pruning, model fusion, and preference alignment.

## News

**[Feb 2025]** ðŸ”¥ðŸ”¥ [FuseChat-3.0](https://arxiv.org/pdf/2503.04222) is accepted to ICLR 2025 Workshop!

**[Jan 2025]** ðŸ”¥ðŸ”¥ We release [FuseO1-Preview](https://huggingface.co/collections/FuseAI/fuseo1-preview-678eb56093649b2688bc9977), an advanced fusion model that enhances System-II reasoning by integrating multiple O1-like models using [SCE](https://arxiv.org/abs/2408.07990) merging, excelling in mathematics, coding, and science.

**[Dec 2024]** ðŸ”¥ We release [FuseChat-3.0](https://huggingface.co/collections/FuseAI/fusechat-30-6752d18dec430bad7a236a75) and [Blog Post](https://slit-ai.github.io/FuseChat-3.0/). FuseChat-3.0 contains a series of models crafted to enhance performance by integrating the strengths of multiple source LLMs into more compact target LLMs.

**[Aug 2024]** ðŸ”¥ We update the [FuseChat tech report](https://arxiv.org/abs/2408.07990) and release [FuseChat-7B-v2.0](https://huggingface.co/FuseAI/FuseChat-7B-v2.0), which is the fusion of six prominent chat LLMs with diverse architectures and scales.  FuseChat-7B-v2.0 achieves an average performance of **7.38** on MT-Bench (GPT-4-0125-Preview as judge LLM), which is comparable to Mixtral-8x7B-Instruct and approaches GPT-3.5-Turbo-1106.

{% include_relative _includes/publications.md %}

{% include_relative _includes/services.md %}
